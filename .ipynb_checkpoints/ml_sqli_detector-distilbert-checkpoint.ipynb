{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic classifier that uses Distilbert model to predict if the payload is an SQL injection\n",
    "##### Distilbert is a smaller and faster version of BERT ( Bidirectional Encoder Representations from Transformers) that is 40% lighter while retaining 97% of BERT's language understanding ability. More on Distilbert at [HuggingFace](https://huggingface.co/docs/transformers/model_doc/distilbert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To begin, let's install and import some packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9o-dl8wLv3Ep"
   },
   "outputs": [],
   "source": [
    "!pip3 install scikit-learn>=1.0.0\n",
    "!pip3 install ktrain matplotlib tensorflow numpy\n",
    "import matplotlib\n",
    "import os\n",
    "import numpy as np\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some more imports...We are using ktrain wrapper to simplify model operations and take advantage of some cool stuff like simplified data set preprocessing, learning rate finding and \"autofit\" that ensures the model is not overfit. More details on ktrain here: [ktrain on GitHub](https://github.com/amaiya/ktrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evreWp_bv3Es"
   },
   "outputs": [],
   "source": [
    "import ktrain\n",
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's print the list of available text classifiers in ktrain. There are relatively simple models like fasttext or bigru that have only 7-10 layers, as well as some more sophisticated deep models like BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.print_text_classifiers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Shmkj8niv3Ev"
   },
   "source": [
    "### Here, we will load our data set. \n",
    "##### We have a CSV file ``trainlist_22k.csv`` that contains a list of HTTP paths that are labeled according to their association with cross-site scripting (xss) and sql injection (sqli).  There is also a \"regular\" traffic that belongs to a \"benign\" class. Data set load is performed using the ```texts_from_csv``` method, which assumes the label_columns are already one-hot-encoded in the spreadsheet. Since *val_filepath* is None, 10% of the data will automatically be used as a validation set.\n",
    "##### In our set we have: 1 feature (payload), 1 label (type) that contains 3 classes:\n",
    " - xss\n",
    " - sqli\n",
    " - benign\n",
    "\n",
    "##### We will be using Distilbert model so preprocessing mode is set to ``Distilbert``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j6-wA79wv3Ew",
    "outputId": "7f7ba49d-d58f-4cf3-f363-806f37d2bd99"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'trainlist_22k.csv'\n",
    "NUM_WORDS = 50000\n",
    "MAXLEN = 200\n",
    "trn, val, preproc = text.texts_from_csv(DATA_PATH,\n",
    "                      'payload',\n",
    "                      label_columns = [\"type\"],\n",
    "                      val_filepath=None, # if None, 10% of data will be used for validation\n",
    "                      max_features=NUM_WORDS, maxlen=MAXLEN,\n",
    "                      ngram_range=1,\n",
    "                      preprocess_mode='distilbert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load the learner instance that uses ```Distilbert``` model. We will retain the model structure unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "UpbaZ68Dv3Ew",
    "outputId": "68874f80-6a80-4960-a152-1b594f4ee856"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, GaussianDropout\n",
    "\n",
    "def get_model():\n",
    "    model = text.text_classifier('distilbert', (trn), \n",
    "                             preproc=preproc)\n",
    "    #model.add(Dense(3, activation='sigmoid'))\n",
    "    #model.add(GaussianDropout1D(0.2))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "model = get_model()\n",
    "learner = ktrain.get_learner(model, train_data=(trn), val_data=(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is what our model looks like. It has a number of layers that are pre-trained therefore allowing us to leverage transfer learning.\n",
    "##### Source code for modeling_tf_distilbert can be found at [HuggingFace Transformers](https://huggingface.co/transformers/v2.3.0/_modules/transformers/modeling_tf_distilbert.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fh1_l7ioIpS6",
    "outputId": "c35e8957-540f-42a7-fd07-d9d417915873"
   },
   "outputs": [],
   "source": [
    "learner.print_layers()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to ensure that majority of existing pre-trained layers are not re-trained so we are freezing those with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ATgiIbyE7XJ"
   },
   "outputs": [],
   "source": [
    "learner.freeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The optimal learning rate for this model can be found using the **lr_find** function however it will take at least **20 minutes!** on this VM that uses CPU only. ( Optimal rate was found to be 3e-5 and therefore there is no need to spend time on this now). **If you still want to proceed**, uncomment the command and run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "u-kza8Env3Ex",
    "outputId": "146052d1-5d60-4708-b878-1660db206379"
   },
   "outputs": [],
   "source": [
    "#learner.lr_find(show_plot=True, max_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's train the model using the optimal learning rate. More accuracy is achieved after 4-5 epochs however to save time, we will run the cycle using 2 epochs only. That should give us ~93% accuracy and observed loss (binary crossentropy) of ~0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_onecycle(3e-5, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Autofit function can help optimally train the model without ``overfitting`` it. **Do not run** unless you are willing to spend days (or perhaps weeks) on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MmjJwfrQv3Ex",
    "outputId": "17265489-ad02-4108-cb8d-549c06ff93f6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#learner.autofit(3e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alright, let's save our predictor so we can use it to perform inferences outside of the Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = ktrain.get_predictor(learner.model, preproc)\n",
    "predictor.save('/home/jupyter/detector_model_lab')\n",
    "print('MODEL SAVED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's time for some fun! First, get a predictor instance that uses our pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = ktrain.load_predictor('/home/jupyter/detector_model_lab')\n",
    "new_model = ktrain.get_predictor(predictor.model, predictor.preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see if it can catch an XSS payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '<applet onkeydown=\"alert(1)\" contenteditable>test</applet>'\n",
    "result = new_model.predict(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can run more serious testing outside of the notebook"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ml_sqli_detector.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
